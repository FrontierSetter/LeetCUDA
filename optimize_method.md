# LeetCUDA é¡¹ç›® Kernel ä¼˜åŒ–æ–¹æ³•æ€»ç»“

## ğŸ“– ç›®å½•
- [1. æŒ‰éš¾åº¦çº§åˆ«çš„ä¼˜åŒ–æ–¹æ³•](#1-æŒ‰éš¾åº¦çº§åˆ«çš„ä¼˜åŒ–æ–¹æ³•)
  - [1.1 â­ï¸ Easy çº§åˆ« (åŸºç¡€æ“ä½œ)](#11-â­ï¸-easy-çº§åˆ«-åŸºç¡€æ“ä½œ)
  - [1.2 â­ï¸â­ï¸ Medium çº§åˆ« (è¿›é˜¶ä¼˜åŒ–)](#12-â­ï¸â­ï¸-medium-çº§åˆ«-è¿›é˜¶ä¼˜åŒ–)
  - [1.3 â­ï¸â­ï¸â­ï¸ Hard çº§åˆ« (Tensor Core)](#13-â­ï¸â­ï¸â­ï¸-hard-çº§åˆ«-tensor-core)
  - [1.4 â­ï¸â­ï¸â­ï¸â­ï¸ Hard+ çº§åˆ« (é«˜çº§ä¼˜åŒ–)](#14-â­ï¸â­ï¸â­ï¸â­ï¸-hard-çº§åˆ«-é«˜çº§ä¼˜åŒ–)
  - [1.5 â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸ Hard++ çº§åˆ« (å‰æ²¿æŠ€æœ¯)](#15-â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸-hard-çº§åˆ«-å‰æ²¿æŠ€æœ¯)
- [2. æ ¸å¿ƒä¼˜åŒ–æŠ€æœ¯è¯¦è§£](#2-æ ¸å¿ƒä¼˜åŒ–æŠ€æœ¯è¯¦è§£)
  - [2.1 å†…å­˜è®¿é—®ä¼˜åŒ–](#21-å†…å­˜è®¿é—®ä¼˜åŒ–)
  - [2.2 è®¡ç®—ä¼˜åŒ–](#22-è®¡ç®—ä¼˜åŒ–)
  - [2.3 æµæ°´çº¿ä¼˜åŒ–](#23-æµæ°´çº¿ä¼˜åŒ–)
- [3. æ€§èƒ½ä¼˜åŒ–é€šç”¨æ¨¡å¼](#3-æ€§èƒ½ä¼˜åŒ–é€šç”¨æ¨¡å¼)
- [4. å…³é”®å®ç°æ€è·¯æ€»ç»“](#4-å…³é”®å®ç°æ€è·¯æ€»ç»“)
- [5. å®é™…æ€§èƒ½æ•°æ®](#5-å®é™…æ€§èƒ½æ•°æ®)

## 1. æŒ‰éš¾åº¦çº§åˆ«çš„ä¼˜åŒ–æ–¹æ³•

### 1.1 â­ï¸ Easy çº§åˆ« (åŸºç¡€æ“ä½œ)

**ç‰¹å¾**: ç›´æ¥å®ç°ç®—æ³•é€»è¾‘ï¼Œæœ€å°åŒ–ä¼˜åŒ–ï¼Œæ³¨é‡åŠŸèƒ½æ­£ç¡®æ€§

**åŒ…å«çš„ kernels**:
- Elementwise æ“ä½œ: `elementwise_add`, `elementwise_mul`
- æ¿€æ´»å‡½æ•°: `relu`, `sigmoid`, `gelu`, `swish`
- ç®€å•æ•°å­¦è¿ç®—: `dot_product`, `embedding`
- åŸºç¡€å˜æ¢: `mat_transpose`

**æ ¸å¿ƒä¼˜åŒ–æ–¹æ³•**:

#### 1.1.1 å‘é‡åŒ–è®¿é—® (Vec4)
```cpp
// 128ä½å‘é‡åŒ–åŠ è½½å­˜å‚¨
#define LDST128BITS(value) (reinterpret_cast<float4*>(&(value))[0])

__global__ void elementwise_add_f32x4_kernel(float *a, float *b, float *c, int N) {
  int idx = 4 * (blockIdx.x * blockDim.x + threadIdx.x);
  if (idx < N) {
    float4 reg_a = FLOAT4(a[idx]);  // 128ä½å‘é‡åŒ–åŠ è½½
    float4 reg_b = FLOAT4(b[idx]);
    float4 reg_c;
    reg_c.x = reg_a.x + reg_b.x;
    reg_c.y = reg_a.y + reg_b.y;
    reg_c.z = reg_a.z + reg_b.z;
    reg_c.w = reg_a.w + reg_b.w;
    FLOAT4(c[idx]) = reg_c;  // 128ä½å‘é‡åŒ–å­˜å‚¨
  }
}
```

#### 1.1.2 å†…å­˜å¯¹é½ä¼˜åŒ–
```cpp
// ç¡®ä¿16å­—èŠ‚å¯¹é½è®¿é—®
__global__ void aligned_access_kernel(half *input, half *output, int N) {
  // ä½¿ç”¨ half8 (128ä½) è¿›è¡Œå¯¹é½è®¿é—®
  int idx = 8 * (blockIdx.x * blockDim.x + threadIdx.x);
  if (idx < N) {
    half8 reg_data = LDST128BITS(input[idx]);  // 128ä½åŠ è½½8ä¸ªhalf
    // ... å¤„ç†æ•°æ®
    LDST128BITS(output[idx]) = reg_data;       // 128ä½å­˜å‚¨
  }
}
```

#### 1.1.3 åˆ†æ”¯æ¶ˆé™¤
```cpp
// ä½¿ç”¨ predication è€Œéæ¡ä»¶åˆ†æ”¯
__global__ void relu_f16x8_pack_kernel(half *x, half *y, int N) {
  int idx = 8 * (blockIdx.x * blockDim.x + threadIdx.x);
  const half2 z2 = {__float2half(0.0f), __float2half(0.0f)};
  half pack_x[8], pack_y[8];

  LDST128BITS(pack_x[0]) = LDST128BITS(x[idx]); // 128ä½åŠ è½½

#pragma unroll
  for (int i = 0; i < 8; i += 2) {
    HALF2(pack_y[i]) = __hmax2(HALF2(pack_x[i]), z2);  // predication
  }
}
```

**æ€§èƒ½æå‡**: 2-3x (ç›¸æ¯”æ ‡é‡ç‰ˆæœ¬)

### 1.2 â­ï¸â­ï¸ Medium çº§åˆ« (è¿›é˜¶ä¼˜åŒ–)

**ç‰¹å¾**: å¼•å…¥ç‰¹å®šä¼˜åŒ–ç­–ç•¥ï¼Œæ˜¾è‘—æå‡æ€§èƒ½

**åŒ…å«çš„ kernels**:
- å½’ä¸€åŒ–å±‚: `layer_norm`, `rms_norm`
- Softmax: `softmax`, `online_softmax`
- ç‚¹ç§¯è¿ç®—: `dot_product` (ä¼˜åŒ–ç‰ˆæœ¬)
- RoPE: æ—‹è½¬ä½ç½®ç¼–ç 
- NMS: éæå¤§å€¼æŠ‘åˆ¶

**æ ¸å¿ƒä¼˜åŒ–æ–¹æ³•**:

#### 1.2.1 åˆ†å—å½’çº¦ (Block All Reduce)
```cpp
template <typename T>
__global__ void block_all_reduce_f16x8_pack_kernel(half* input, half* output, int N) {
  extern __shared__ half sdata[];

  int tid = threadIdx.x;
  int bid = blockIdx.x;
  int block_size = blockDim.x;

  // 1. Load data with vectorization
  int idx = bid * block_size * 8 + tid * 8;
  half8 reg_input = LDST128BITS(input[idx]);  // 128ä½åŠ è½½

  // 2. Thread-level reduction
  half8 partial_sum = reg_input;
  for (int offset = 1; offset < block_size; offset *= 2) {
    __syncthreads();
    int neighbor = tid - offset;
    if (neighbor >= 0) {
      half8 neighbor_data = sdata[neighbor * 8];
      partial_sum = __hadd8(partial_sum, neighbor_data);
    }
  }

  // 3. Store result
  if (tid == 0) {
    LDST128BITS(output[bid * 8]) = partial_sum;
  }
}
```

#### 1.2.2 å±‚å½’ä¸€åŒ–ä¼˜åŒ–
```cpp
__global__ void layer_norm_f16x8_pack_kernel(half* input, half* gamma, half* beta,
                                             half* output, int N, int hidden_size) {
  extern __shared__ float sdata[];

  int tid = threadIdx.x;
  int bid = blockIdx.x;
  int block_size = blockDim.x;

  // 1. è®¡ç®—å‡å€¼å’Œæ–¹å·®
  float sum = 0.0f, sum_sq = 0.0f;
  for (int i = tid; i < hidden_size; i += block_size) {
    float val = __half2float(input[bid * hidden_size + i]);
    sum += val;
    sum_sq += val * val;
  }

  // 2. å½’çº¦
  // ... (å½’çº¦é€»è¾‘)

  // 3. æ ‡å‡†åŒ–
  float mean = sum / hidden_size;
  float var = sum_sq / hidden_size - mean * mean;
  float inv_std = rsqrtf(var + 1e-5f);

  // 4. åº”ç”¨ gamma å’Œ beta
  for (int i = tid; i < hidden_size; i += block_size) {
    float val = (__half2float(input[bid * hidden_size + i]) - mean) * inv_std;
    float gamma_val = __half2float(gamma[i]);
    float beta_val = __half2float(beta[i]);
    output[bid * hidden_size + i] = __float2half(val * gamma_val + beta_val);
  }
}
```

#### 1.2.3 Softmax åœ¨çº¿è®¡ç®—
```cpp
__global__ void online_softmax_f32x4_pack_kernel(float* input, float* output, int N) {
  extern __shared__ float sdata[];

  int tid = threadIdx.x;
  int bid = blockIdx.x;
  int block_size = blockDim.x;

  // 1. è®¡ç®—æœ€å¤§å€¼ (æ•°å€¼ç¨³å®šæ€§)
  float max_val = -INFINITY_F;
  for (int i = tid; i < N; i += block_size) {
    max_val = fmaxf(max_val, input[bid * N + i]);
  }

  // 2. å½’çº¦æœ€å¤§å€¼
  // ... (å½’çº¦é€»è¾‘)

  // 3. è®¡ç®—æŒ‡æ•°å’Œ
  float sum_exp = 0.0f;
  for (int i = tid; i < N; i += block_size) {
    float exp_val = expf(input[bid * N + i] - max_val);
    sdata[tid] = exp_val;
    sum_exp += exp_val;
  }

  // 4. å½’çº¦æ±‚å’Œ
  // ... (å½’çº¦é€»è¾‘)

  // 5. è®¡ç®—æœ€ç»ˆç»“æœ
  for (int i = tid; i < N; i += block_size) {
    float exp_val = expf(input[bid * N + i] - max_val);
    output[bid * N + i] = exp_val / sum_exp;
  }
}
```

**æ€§èƒ½æå‡**: 5-10x (ç›¸æ¯”åŸºç¡€ç‰ˆæœ¬)

### 1.3 â­ï¸â­ï¸â­ï¸ Hard çº§åˆ« (Tensor Core)

**ç‰¹å¾**: ä½¿ç”¨ Tensor Core å’Œ MMA æŒ‡ä»¤ï¼Œå¤æ‚å†…å­˜ç®¡ç†

**åŒ…å«çš„ kernels**:
- SGEMV: å•ç²¾åº¦çŸ©é˜µå‘é‡ä¹˜æ³•
- HGEVM: åŠç²¾åº¦çŸ©é˜µå‘é‡ä¹˜æ³•
- SGEMM: å•ç²¾åº¦çŸ©é˜µä¹˜æ³•
- HGEMM: åŠç²¾åº¦çŸ©é˜µä¹˜æ³• (åŸºç¡€ç‰ˆæœ¬)

**æ ¸å¿ƒä¼˜åŒ–æ–¹æ³•**:

#### 1.3.1 Tensor Core MMA æŒ‡ä»¤ä½¿ç”¨
```cpp
// MMA æŒ‡ä»¤å®å®šä¹‰
#define HMMA16816(RD0, RD1, RA0, RA1, RA2, RA3, RB0, RB1, RC0, RC1) \
  asm volatile( \
    "mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%0, %1}, {%2, %3, %4, %5}, {%6, %7}, {%8, %9};\n" \
    : "=r"(RD0), "=r"(RD1) \
    : "r"(RA0), "r"(RA1), "r"(RA2), "r"(RA3), "r"(RB0), "r"(RB1), "r"(RC0), "r"(RC1))

// HGEMM with Tensor Core
template <const int BM = 128, const int BN = 128, const int BK = 8>
__global__ void hgemm_mma_kernel(half* a, half* b, half* c, int M, int N, int K) {
  // 1. Shared memory tiling
  __shared__ half s_a[BM][BK];
  __shared__ half s_b[BK][BN];

  // 2. Register allocation for MMA
  half frag_a[8];  // 8 registers for A fragment
  half frag_b[4];  // 4 registers for B fragment
  float frag_c[8]; // 8 registers for C fragment (FP32 accumulation)

  // 3. Main computation loop with MMA
  for (int k = 0; k < (K + BK - 1) / BK; ++k) {
    // Load from global memory to shared memory
    // ... (omitted for brevity)

    __syncthreads();

    // Compute with MMA instructions
    #pragma unroll
    for (int kk = 0; kk < BK; ++kk) {
      // Load fragments from shared memory
      // ... (omitted for brevity)

      // Execute MMA instruction
      HMMA16816(frag_c[0], frag_c[1], frag_a[0], frag_a[1], frag_a[2], frag_a[3],
                frag_b[0], frag_b[1], frag_c[0], frag_c[1]);
    }

    __syncthreads();
  }

  // 4. Store result
  // ... (omitted for brevity)
}
```

#### 1.3.2 å¯„å­˜å™¨åˆ†å—ç­–ç•¥
```cpp
template <const int TM = 8, const int TN = 8>
__global__ void register_optimized_kernel(half* A, half* B, half* C, int M, int N, int K) {
  // å¯„å­˜å™¨åˆ†å—
  half reg_a[TM][TN];  // A fragment in registers
  half reg_b[TM][TN];  // B fragment in registers
  float reg_c[TM][TN]; // C fragment in registers (FP32 accumulation)

  // è®¡ç®—å¾ªç¯
  for (int k = 0; k < K; k += TN) {
    // åŠ è½½åˆ°å¯„å­˜å™¨
    #pragma unroll
    for (int i = 0; i < TM; ++i) {
      #pragma unroll
      for (int j = 0; j < TN; ++j) {
        reg_a[i][j] = A[threadIdx.y * TM + i][k + j];
        reg_b[i][j] = B[k + i][threadIdx.x * TN + j];
      }
    }

    // è®¡ç®—
    #pragma unroll
    for (int i = 0; i < TM; ++i) {
      #pragma unroll
      for (int j = 0; j < TN; ++j) {
        reg_c[i][j] += __hmul(reg_a[i][j], reg_b[i][j]);
      }
    }
  }
}
```

#### 1.3.3 å…±äº«å†…å­˜åˆ†å—
```cpp
template <const int BM = 32, const int BN = 32, const int BK = 32>
__global__ void hgemm_sliced_k_f16_kernel(half *a, half *b, half *c, int M, int N, int K) {
  // Shared memory tiles
  __shared__ half s_a[BM][BK];
  __shared__ half s_b[BK][BN];

  int bx = blockIdx.x;
  int by = blockIdx.y;
  int tx = threadIdx.x;
  int ty = threadIdx.y;

  // Load to shared memory
  int load_smem_a_m = ty;
  int load_smem_a_k = tx;
  int load_smem_b_k = ty;
  int load_smem_b_n = tx;

  half sum = __float2half(0.f);
  for (int bk = 0; bk < (K + BK - 1) / BK; ++bk) {
    // Load from global to shared
    s_a[load_smem_a_m][load_smem_a_k] = a[load_gmem_a_m * K + load_gmem_a_k];
    s_b[load_smem_b_k][load_smem_b_n] = b[load_gmem_b_k * N + load_gmem_b_n];
    __syncthreads();

    // Compute
    #pragma unroll
    for (int k = 0; k < BK; ++k) {
      sum += s_a[load_smem_a_m][k] * s_b[k][load_smem_b_n];
    }

    __syncthreads();
  }

  // Store result
  c[store_gmem_c_m * N + store_gmem_c_n] = sum;
}
```

**æ€§èƒ½æå‡**: 10-20x (ç›¸æ¯” Medium ç‰ˆæœ¬)

### 1.4 â­ï¸â­ï¸â­ï¸â­ï¸ Hard+ çº§åˆ« (é«˜çº§ä¼˜åŒ–)

**ç‰¹å¾**: å¤šé˜¶æ®µæµæ°´çº¿ï¼Œå¤æ‚å†…å­˜ä¼˜åŒ–

**åŒ…å«çš„ kernels**:
- FlashAttention-2: å®Œæ•´å®ç°
- é«˜çº§ HGEMM: å¤šé˜¶æ®µç‰ˆæœ¬
- CUTLASS é›†æˆ: ä½¿ç”¨ NVIDIA åº“

**æ ¸å¿ƒä¼˜åŒ–æ–¹æ³•**:

#### 1.4.1 å¤šé˜¶æ®µæµæ°´çº¿
```cpp
template <const int kStage = 2, const int kPad = 8>
__global__ void flash_attn_mma_stages_split_q_kernel(
    half* Q, half* K, half* V, half* O, int QKV_seqlen, int QKV_head) {

  // 1. Multi-stage shared memory
  extern __shared__ half smem[];
  constexpr int Q_tile_size = Br * (kHeadDim + kPad);
  constexpr int KV_tile_size = Bc * (kHeadDim + kPad);
  half* Q_tile_smem = smem;
  half* K_tile_smem = Q_tile_smem + Q_tile_size;
  half* V_tile_smem = K_tile_smem + kStage * KV_tile_size;

  // 2. Asynchronous memory copy
  for (int stage = 0; stage < kStage; ++stage) {
    // Prefetch next tile
    if (stage < kStage - 1) {
      CP_ASYNC_CA(load_smem_K_ptr, load_gmem_K_ptr, bytes);
      CP_ASYNC_CA(load_smem_V_ptr, load_gmem_V_ptr, bytes);
    }

    __syncthreads();

    // Compute with current tile
    // ... (MMA computation)

    // Commit and wait
    CP_ASYNC_COMMIT_GROUP();
    if (stage > 0) {
      CP_ASYNC_WAIT_GROUP(stage - 1);
    }
  }

  // 3. Collective store via warp shuffle
  // ... (omitted for brevity)
}
```

#### 1.4.2 å¼‚æ­¥å†…å­˜æ‹·è´
```cpp
// cp.async å®å®šä¹‰
#define CP_ASYNC_COMMIT_GROUP() asm volatile("cp.async.commit_group;\n" ::)
#define CP_ASYNC_WAIT_ALL() asm volatile("cp.async.wait_all;\n" ::)
#define CP_ASYNC_WAIT_GROUP(n) asm volatile("cp.async.wait_group %0;\n" ::"n"(n))
#define CP_ASYNC_CA(dst, src, bytes) \
  asm volatile("cp.async.ca.shared.global.L2::128B [%0], [%1], %2;\n" ::"r"(dst), "l"(src), "n"(bytes))

// ä½¿ç”¨ç¤ºä¾‹
__global__ void async_copy_kernel(half* gmem_input, half* smem_output, int N) {
  // å¼‚æ­¥æ‹·è´åˆ°å…±äº«å†…å­˜
  CP_ASYNC_CA(smem_output, gmem_input, N * sizeof(half));

  // æäº¤ç»„
  CP_ASYNC_COMMIT_GROUP();

  // ç­‰å¾…æ‰€æœ‰å¼‚æ­¥æ“ä½œå®Œæˆ
  CP_ASYNC_WAIT_ALL();
}
```

#### 1.4.3 å¯„å­˜å™¨åŒç¼“å†²
```cpp
template <const int kStage = 2>
__global__ void double_buffer_kernel(half* input, half* output, int N) {
  extern __shared__ half smem[];

  // åŒç¼“å†²å¯„å­˜å™¨
  half reg_buffer[2][BLOCK_SIZE / 32]; // æ¯ä¸ªwarpä¸€ä¸ªç¼“å†²

  for (int i = 0; i < N; i += BLOCK_SIZE) {
    int stage = (i / BLOCK_SIZE) % kStage;

    // å¼‚æ­¥åŠ è½½åˆ°å¯„å­˜å™¨
    if (i + BLOCK_SIZE < N) {
      CP_ASYNC_CA(&smem[(stage + 1) % kStage * BLOCK_SIZE],
                  &input[i + BLOCK_SIZE],
                  BLOCK_SIZE * sizeof(half));
    }

    // ä»å¯„å­˜å™¨è®¡ç®—
    compute_with_registers(reg_buffer[stage]);

    // åˆ‡æ¢ç¼“å†²åŒº
    stage = (stage + 1) % kStage;
  }
}
```

**æ€§èƒ½æå‡**: 20-50x (ç›¸æ¯” Hard ç‰ˆæœ¬)

### 1.5 â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸ Hard++ çº§åˆ« (å‰æ²¿æŠ€æœ¯)

**ç‰¹å¾**: æœ€æ–°ä¼˜åŒ–æŠ€æœ¯ï¼Œé’ˆå¯¹ç‰¹å®šç¡¬ä»¶ä¼˜åŒ–

**åŒ…å«çš„ kernels**:
- FFPA Attention: æ›´å¿«çš„ Flash Prefill Attention
- é«˜çº§ Swizzle: æ‰‹åŠ¨å†…å­˜äº¤é”™ä¼˜åŒ–
- æ··åˆç²¾åº¦: FP16/FP32 æ··åˆè®¡ç®—

**æ ¸å¿ƒä¼˜åŒ–æ–¹æ³•**:

#### 1.5.1 FFPA Attention (O(1) SRAM å¤æ‚åº¦)
```cpp
// Fine-grained tiling for constant SRAM usage
template <const int kMmaAtomK = 16>
__global__ void ffpa_tiling_qkv_kernel(half* Q, half* K, half* V, half* O,
                                      int QKV_seqlen, int QKV_head) {
  // SRAM complexity: O(16 * kMmaAtomK) = O(256) constant
  constexpr int SRAM_Q = 16 * kMmaAtomK;    // 256 elements
  constexpr int SRAM_KV = 16 * kMmaAtomK;   // 256 elements

  extern __shared__ half smem[];
  half* s_Q = smem;
  half* s_K = s_Q + SRAM_Q;
  half* s_V = s_K + SRAM_KV;

  // Fine-grained tiling across MMA level
  // ... (omitted for brevity)
}
```

#### 1.5.2 é«˜çº§ Swizzle (Bank Conflict Free)
```cpp
// Swizzle å‡½æ•°ï¼šé¿å… bank conflicts
__device__ __host__ __forceinline__ int swizzle_j(int i, int j) {
  return ((int(j / 8) ^ int(i / 4)) % 2) * 8;
}

// åº”ç”¨ç¤ºä¾‹
__global__ void swizzle_kernel(half* A, half* B, half* C, int M, int N, int K) {
  // ... è®¡ç®—é€»è¾‘
  int smem_addr = i * (N + 8) + swizzle_j(i, j);  // åº”ç”¨ swizzle
  sdata[smem_addr] = value;
}

// æ‰‹åŠ¨ swizzle å®ç°
__global__ void manual_swizzle_kernel(half* input, half* output, int M, int N) {
  extern __shared__ half sdata[];

  int tx = threadIdx.x;
  int ty = threadIdx.y;

  // æ‰‹åŠ¨ swizzle åœ°å€è®¡ç®—
  int swizzle_offset = ((tx / 8) ^ (ty / 4)) % 2 * 8;
  int smem_addr = ty * (N + 8) + tx + swizzle_offset;

  sdata[smem_addr] = input[threadIdx.y * N + threadIdx.x];
  __syncthreads();

  // è¯»å–æ—¶ä¹Ÿéœ€è¦ç›¸åŒçš„ swizzle
  output[threadIdx.y * N + threadIdx.x] = sdata[smem_addr];
}
```

#### 1.5.3 æ··åˆç²¾åº¦è®¡ç®—
```cpp
// æ··åˆç²¾åº¦ FFPA
__global__ void mixed_precision_ffpa_kernel(half* Q, half* K, half* V, half* O,
                                           int QKV_seqlen, int QKV_head) {
  // QK ä½¿ç”¨ FP32 ç²¾åº¦ï¼ŒPV ä½¿ç”¨ FP16 ç²¾åº¦
  float reg_qk[16];  // FP32 for QK computation
  half reg_pv[16];   // FP16 for PV computation

  // QK è®¡ç®— (FP32)
  #pragma unroll
  for (int i = 0; i < 16; ++i) {
    float q_val = __half2float(Q[i]);
    float k_val = __half2float(K[i]);
    reg_qk[i] = q_val * k_val;  // FP32 multiplication
  }

  // PV è®¡ç®— (FP16)
  #pragma unroll
  for (int i = 0; i < 16; ++i) {
    half p_val = __float2half(reg_qk[i]);  // Convert to FP16
    half v_val = V[i];
    reg_pv[i] = __hmul(p_val, v_val);      // FP16 multiplication
  }
}
```

**æ€§èƒ½æå‡**: 50-100x (ç›¸æ¯”åŸºç¡€ç‰ˆæœ¬)

## 2. æ ¸å¿ƒä¼˜åŒ–æŠ€æœ¯è¯¦è§£

### 2.1 å†…å­˜è®¿é—®ä¼˜åŒ–

#### 2.1.1 å‘é‡åŒ–è®¿é—®æ¨¡å¼
```cpp
// 32ä½å‘é‡åŒ– (half2)
#define LDST32BITS(value) (reinterpret_cast<half2*>(&(value))[0])

// 64ä½å‘é‡åŒ– (float2)
#define LDST64BITS(value) (reinterpret_cast<float2*>(&(value))[0])

// 128ä½å‘é‡åŒ– (float4/half8)
#define LDST128BITS(value) (reinterpret_cast<float4*>(&(value))[0])

// ä½¿ç”¨ç¤ºä¾‹
__global__ void vectorized_kernel(half* input, half* output, int N) {
  int idx = 8 * (blockIdx.x * blockDim.x + threadIdx.x);
  half8 reg_data = LDST128BITS(input[idx]);  // 128ä½åŠ è½½8ä¸ªhalf
  // ... å¤„ç†æ•°æ®
  LDST128BITS(output[idx]) = reg_data;       // 128ä½å­˜å‚¨
}
```

#### 2.1.2 å†…å­˜åˆå¹¶è®¿é—®
```cpp
// ç¡®ä¿åŒwarpå†…çº¿ç¨‹è®¿é—®è¿ç»­å†…å­˜
__global__ void coalesced_access_kernel(float* input, float* output, int N) {
  int warp_id = threadIdx.x / 32;
  int lane_id = threadIdx.x % 32;
  int base_addr = blockIdx.x * blockDim.x * 4;  // 4å­—èŠ‚å¯¹é½

  // åŒwarpå†…çº¿ç¨‹è®¿é—®è¿ç»­åœ°å€
  float4 data = reinterpret_cast<float4*>(input + base_addr)[lane_id];

  // å¤„ç†æ•°æ®
  // ... (è®¡ç®—é€»è¾‘)

  // å­˜å‚¨ç»“æœ
  reinterpret_cast<float4*>(output + base_addr)[lane_id] = data;
}
```

#### 2.1.3 é“¶è¡Œå†²çªé¿å…
```cpp
// Bank conflict free shared memory layout
template <const int kBankWidth = 32>
__global__ void bank_conflict_free_kernel(half* input, half* output, int M, int N) {
  extern __shared__ half sdata[];

  int tx = threadIdx.x;
  int ty = threadIdx.y;

  // æ·»åŠ  padding é¿å… bank conflicts
  constexpr int kPadding = 8;  // 8ä¸ªhalfçš„padding
  int smem_addr = ty * (N + kPadding) + tx;

  // Load with padding
  sdata[smem_addr] = input[ty * N + tx];
  __syncthreads();

  // Process data
  // ... (è®¡ç®—é€»è¾‘)

  // Store result
  output[ty * N + tx] = sdata[smem_addr];
}
```

### 2.2 è®¡ç®—ä¼˜åŒ–

#### 2.2.1 Tensor Core ä½¿ç”¨
```cpp
// m16n8k16 MMA æŒ‡ä»¤
#define HMMA16816(RD0, RD1, RA0, RA1, RA2, RA3, RB0, RB1, RC0, RC1) \
  asm volatile( \
    "mma.sync.aligned.m16n8k16.row.col.f16.f16.f16.f16 {%0, %1}, {%2, %3, %4, %5}, {%6, %7}, {%8, %9};\n" \
    : "=r"(RD0), "=r"(RD1) \
    : "r"(RA0), "r"(RA1), "r"(RA2), "r"(RA3), "r"(RB0), "r"(RB1), "r"(RC0), "r"(RC1))

// ä½¿ç”¨ç¤ºä¾‹
__global__ void mma_compute_kernel(half* A, half* B, half* C, int M, int N, int K) {
  // å‡†å¤‡å¯„å­˜å™¨
  half RA[4], RB[2], RC[2];
  float RD[2];

  // æ‰§è¡Œ MMA
  HMMA16816(RD[0], RD[1], RA[0], RA[1], RA[2], RA[3], RB[0], RB[1], RC[0], RC[1]);
}
```

#### 2.2.2 å¯„å­˜å™¨åˆ†é…ä¼˜åŒ–
```cpp
// å¯„å­˜å™¨åˆ†å—ç­–ç•¥
template <const int TM = 16, const int TN = 8>
__global__ void register_tiling_kernel(half* A, half* B, half* C, int M, int N, int K) {
  // å¯„å­˜å™¨åˆ†å—
  half reg_a[TM][TN];
  half reg_b[TM][TN];
  float reg_c[TM][TN];

  // è®¡ç®—å¾ªç¯
  for (int k = 0; k < K; k += TN) {
    // åŠ è½½åˆ°å¯„å­˜å™¨
    #pragma unroll
    for (int i = 0; i < TM; ++i) {
      #pragma unroll
      for (int j = 0; j < TN; ++j) {
        reg_a[i][j] = A[threadIdx.y * TM + i][k + j];
        reg_b[i][j] = B[k + i][threadIdx.x * TN + j];
      }
    }

    // è®¡ç®—
    #pragma unroll
    for (int i = 0; i < TM; ++i) {
      #pragma unroll
      for (int j = 0; j < TN; ++j) {
        reg_c[i][j] += __hmul(reg_a[i][j], reg_b[i][j]);
      }
    }
  }
}
```

#### 2.2.3 å¾ªç¯å±•å¼€å’Œ unroll
```cpp
// å¾ªç¯å±•å¼€ä¼˜åŒ–
template <const int UNROLL_FACTOR = 4>
__global__ void unrolled_kernel(half* input, half* output, int N) {
  int tid = blockIdx.x * blockDim.x + threadIdx.x;

  // æ‰‹åŠ¨å¾ªç¯å±•å¼€
  #pragma unroll UNROLL_FACTOR
  for (int i = tid; i < N; i += blockDim.x * gridDim.x * UNROLL_FACTOR) {
    // ç¬¬ä¸€æ¬¡è¿­ä»£
    output[i] = __hadd(input[i], __float2half(1.0f));

    // ç¬¬äºŒæ¬¡è¿­ä»£
    if (i + blockDim.x * gridDim.x < N) {
      output[i + blockDim.x * gridDim.x] =
        __hadd(input[i + blockDim.x * gridDim.x], __float2half(1.0f));
    }

    // ç¬¬ä¸‰æ¬¡è¿­ä»£
    if (i + 2 * blockDim.x * gridDim.x < N) {
      output[i + 2 * blockDim.x * gridDim.x] =
        __hadd(input[i + 2 * blockDim.x * gridDim.x], __float2half(1.0f));
    }

    // ç¬¬å››æ¬¡è¿­ä»£
    if (i + 3 * blockDim.x * gridDim.x < N) {
      output[i + 3 * blockDim.x * gridDim.x] =
        __hadd(input[i + 3 * blockDim.x * gridDim.x], __float2half(1.0f));
    }
  }
}
```

### 2.3 æµæ°´çº¿ä¼˜åŒ–

#### 2.3.1 å¤šé˜¶æ®µæµæ°´çº¿
```cpp
template <const int kStage = 3>
__global__ void pipeline_kernel(half* input, half* output, int N) {
  extern __shared__ half smem[];

  // é˜¶æ®µç¼“å†²åŒº
  constexpr int BUFFER_SIZE = BLOCK_SIZE;
  half* buffers[kStage];

  for (int s = 0; s < kStage; ++s) {
    buffers[s] = smem + s * BUFFER_SIZE;
  }

  int stage = 0;

  // ä¸»å¾ªç¯
  for (int i = 0; i < N; i += kStage * BLOCK_SIZE) {
    // 1. é¢„å–ä¸‹ä¸€é˜¶æ®µæ•°æ®
    if (i + (stage + 1) * BLOCK_SIZE < N) {
      CP_ASYNC_CA(buffers[(stage + 1) % kStage],
                  &input[i + (stage + 1) * BLOCK_SIZE],
                  BLOCK_SIZE * sizeof(half));
    }

    // 2. è®¡ç®—å½“å‰é˜¶æ®µ
    __syncthreads();
    compute_stage(buffers[stage], &output[i]);

    // 3. åŒæ­¥å’Œåˆ‡æ¢é˜¶æ®µ
    CP_ASYNC_COMMIT_GROUP();
    if (i > 0) {
      CP_ASYNC_WAIT_GROUP((stage - 1 + kStage) % kStage);
    }

    stage = (stage + 1) % kStage;
  }
}
```

#### 2.3.2 å¼‚æ­¥å†…å­˜æ‹·è´
```cpp
// å¼‚æ­¥æ‹·è´å®
#define CP_ASYNC_CA(dst, src, bytes) \
  asm volatile("cp.async.ca.shared.global.L2::128B [%0], [%1], %2;\n" ::"r"(dst), "l"(src), "n"(bytes))
#define CP_ASYNC_CG(dst, src, bytes) \
  asm volatile("cp.async.cg.shared.global.L2::128B [%0], [%1], %2;\n" ::"r"(dst), "l"(src), "n"(bytes))

// ä½¿ç”¨ç¤ºä¾‹
__global__ void async_pipeline_kernel(half* gmem_input, half* smem_output, int N) {
  // å¼‚æ­¥æ‹·è´å¤šä¸ªå—
  for (int i = 0; i < N; i += BLOCK_SIZE * 2) {
    // å‘èµ·å¤šä¸ªå¼‚æ­¥æ‹·è´
    CP_ASYNC_CA(&smem_output[0], &gmem_input[i], BLOCK_SIZE * sizeof(half));
    CP_ASYNC_CA(&smem_output[BLOCK_SIZE], &gmem_input[i + BLOCK_SIZE], BLOCK_SIZE * sizeof(half));

    // æäº¤ç»„
    CP_ASYNC_COMMIT_GROUP();

    // è®¡ç®—ç¬¬ä¸€ä¸ªå—
    compute_block(&smem_output[0]);

    // ç­‰å¾…ç¬¬äºŒä¸ªå—å®Œæˆ
    CP_ASYNC_WAIT_GROUP(0);

    // è®¡ç®—ç¬¬äºŒä¸ªå—
    compute_block(&smem_output[BLOCK_SIZE]);
  }
}
```

#### 2.3.3 å¯„å­˜å™¨åŒç¼“å†²
```cpp
template <const int kStage = 2>
__global__ void double_buffer_kernel(half* input, half* output, int N) {
  // åŒç¼“å†²å¯„å­˜å™¨
  half reg_buffer[2][BLOCK_SIZE / 32]; // æ¯ä¸ªwarpä¸€ä¸ªç¼“å†²

  for (int i = 0; i < N; i += BLOCK_SIZE) {
    int current_stage = (i / BLOCK_SIZE) % kStage;
    int next_stage = (current_stage + 1) % kStage;

    // å¼‚æ­¥åŠ è½½åˆ°å¯„å­˜å™¨
    if (i + BLOCK_SIZE < N) {
      CP_ASYNC_CA(Â®_buffer[next_stage], &input[i + BLOCK_SIZE], BLOCK_SIZE * sizeof(half));
    }

    // ä»å¯„å­˜å™¨è®¡ç®—
    compute_with_registers(reg_buffer[current_stage]);

    // åˆ‡æ¢ç¼“å†²åŒº
    current_stage = next_stage;
  }
}
```

## 3. æ€§èƒ½ä¼˜åŒ–é€šç”¨æ¨¡å¼

### 3.1 å†…å­˜å±‚æ¬¡ä¼˜åŒ–ç­–ç•¥

#### 3.1.1 å¯„å­˜å™¨ä¼˜å…ˆåŸåˆ™
```cpp
// æœ€å¤§åŒ–å¯„å­˜å™¨ä½¿ç”¨
template <const int kRegisterCount = 128>
__global__ void register_heavy_kernel(half* input, half* output, int N) {
  // ä½¿ç”¨å¤§é‡å¯„å­˜å™¨å‡å°‘å†…å­˜è®¿é—®
  half reg_data[kRegisterCount];

  // é¢„åŠ è½½åˆ°å¯„å­˜å™¨
  #pragma unroll
  for (int i = 0; i < kRegisterCount; ++i) {
    int idx = blockIdx.x * blockDim.x * kRegisterCount + threadIdx.x * kRegisterCount + i;
    if (idx < N) {
      reg_data[i] = input[idx];
    }
  }

  // åœ¨å¯„å­˜å™¨ä¸­è®¡ç®—
  #pragma unroll
  for (int i = 0; i < kRegisterCount; ++i) {
    reg_data[i] = __hadd(reg_data[i], __float2half(1.0f));
  }

  // å­˜å‚¨ç»“æœ
  #pragma unroll
  for (int i = 0; i < kRegisterCount; ++i) {
    int idx = blockIdx.x * blockDim.x * kRegisterCount + threadIdx.x * kRegisterCount + i;
    if (idx < N) {
      output[idx] = reg_data[i];
    }
  }
}
```

#### 3.1.2 å…±äº«å†…å­˜ä¼˜åŒ–
```cpp
// å…±äº«å†…å­˜æœ€ä½³å®è·µ
template <const int TILE_SIZE = 32>
__global__ void shared_memory_optimized_kernel(half* input, half* output, int M, int N) {
  extern __shared__ half sdata[];

  int tx = threadIdx.x;
  int ty = threadIdx.y;

  // 1. åŠ è½½æ•°æ®åˆ°å…±äº«å†…å­˜ (è€ƒè™‘ bank conflicts)
  int smem_addr = ty * (TILE_SIZE + 8) + tx;  // +8 padding
  sdata[smem_addr] = input[ty * N + tx];

  __syncthreads();

  // 2. è®¡ç®— (æœ€å¤§åŒ–å…±äº«å†…å­˜é‡ç”¨)
  #pragma unroll
  for (int i = 0; i < 4; ++i) {
    half val = sdata[smem_addr];
    // ... è®¡ç®—é€»è¾‘
    sdata[smem_addr] = val;
  }

  __syncthreads();

  // 3. å­˜å‚¨ç»“æœ
  output[ty * N + tx] = sdata[smem_addr];
}
```

#### 3.1.3 å…¨å±€å†…å­˜ä¼˜åŒ–
```cpp
// å…¨å±€å†…å­˜è®¿é—®ä¼˜åŒ–
__global__ void global_memory_optimized_kernel(half* input, half* output, int N) {
  // 1. ç¡®ä¿å†…å­˜å¯¹é½
  int aligned_idx = (blockIdx.x * blockDim.x + threadIdx.x) * 8;  // 8ä¸ªhalfå¯¹é½

  // 2. å‘é‡åŒ–è®¿é—®
  if (aligned_idx < N) {
    half8 data = LDST128BITS(input[aligned_idx]);

    // 3. è®¡ç®— (ä½¿ç”¨å‘é‡åŒ–æ“ä½œ)
    half8 result;
    #pragma unroll
    for (int i = 0; i < 8; ++i) {
      result[i] = __hadd(data[i], __float2half(1.0f));
    }

    // 4. å­˜å‚¨ç»“æœ
    LDST128BITS(output[aligned_idx]) = result;
  }
}
```

### 3.2 å¹¶è¡Œä¼˜åŒ–ç­–ç•¥

#### 3.2.1 Warp åˆ©ç”¨ä¼˜åŒ–
```cpp
// ç¡®ä¿ warp å†…æ‰€æœ‰çº¿ç¨‹éƒ½æœ‰å·¥ä½œ
__global__ void warp_optimized_kernel(half* input, half* output, int N) {
  int warp_id = threadIdx.x / 32;
  int lane_id = threadIdx.x % 32;
  int total_warps = (blockDim.x * gridDim.x) / 32;

  // æ¯ä¸ª warp å¤„ç†å›ºå®šæ•°é‡çš„å…ƒç´ 
  int elements_per_warp = (N + total_warps - 1) / total_warps;
  int warp_start = warp_id * elements_per_warp;
  int warp_end = min(warp_start + elements_per_warp, N);

  // warp å†…è´Ÿè½½å‡è¡¡
  for (int i = warp_start + lane_id; i < warp_end; i += 32) {
    output[i] = __hadd(input[i], __float2half(1.0f));
  }
}
```

#### 3.2.2 Occupancy ä¼˜åŒ–
```cpp
// ä½¿ç”¨ __launch_bounds__ æ§åˆ¶èµ„æºä½¿ç”¨
template <const int MAX_THREADS = 256, const int MIN_BLOCKS = 8>
__global__ void __launch_bounds__(MAX_THREADS, MIN_BLOCKS)
occupancy_optimized_kernel(half* input, half* output, int N) {
  // å‡å°‘å¯„å­˜å™¨ä½¿ç”¨ä»¥æé«˜ occupancy
  int tid = blockIdx.x * blockDim.x + threadIdx.x;

  // ä½¿ç”¨æ›´å°‘çš„å¯„å­˜å™¨å˜é‡
  half temp;

  // è®¡ç®— (é¿å…ä½¿ç”¨å¤§é‡ä¸­é—´å˜é‡)
  for (int i = tid; i < N; i += blockDim.x * gridDim.x) {
    temp = input[i];
    temp = __hadd(temp, __float2half(0.5f));
    output[i] = temp;
  }
}
```

#### 3.2.3 å†…å­˜åˆå¹¶è®¿é—®
```cpp
// ç¡®ä¿å†…å­˜è®¿é—®åˆå¹¶
__global__ void coalesced_access_kernel(float* input, float* output, int N) {
  int warp_id = blockIdx.x * (blockDim.x / 32) + threadIdx.x / 32;
  int lane_id = threadIdx.x % 32;

  // æ¯ä¸ª warp å¤„ç†è¿ç»­çš„å†…å­˜å—
  int base_addr = warp_id * 32 * 4;  // 4ä¸ªfloatå¯¹é½

  // åŒwarpå†…çº¿ç¨‹è®¿é—®è¿ç»­åœ°å€
  float4 data = reinterpret_cast<float4*>(input + base_addr)[lane_id];

  // å¤„ç†æ•°æ®
  data.x += 1.0f;
  data.y += 1.0f;
  data.z += 1.0f;
  data.w += 1.0f;

  // å­˜å‚¨ç»“æœ
  reinterpret_cast<float4*>(output + base_addr)[lane_id] = data;
}
```

### 3.3 è®¡ç®—ä¼˜åŒ–ç­–ç•¥

#### 3.3.1 æŒ‡ä»¤çº§å¹¶è¡Œ
```cpp
// é€šè¿‡æµæ°´çº¿æé«˜æŒ‡ä»¤çº§å¹¶è¡Œåº¦
__global__ void instruction_parallel_kernel(half* input, half* output, int N) {
  int tid = blockIdx.x * blockDim.x + threadIdx.x;

  // åˆ›å»ºæŒ‡ä»¤çº§å¹¶è¡Œ
  half reg_a, reg_b, reg_c, reg_d;

  for (int i = tid; i < N; i += blockDim.x * gridDim.x) {
    // åŠ è½½é˜¶æ®µ
    reg_a = input[i];
    reg_b = input[i + 1];
    reg_c = input[i + 2];
    reg_d = input[i + 3];

    // è®¡ç®—é˜¶æ®µ (å¯ä»¥å¹¶è¡Œæ‰§è¡Œ)
    reg_a = __hadd(reg_a, __float2half(1.0f));
    reg_b = __hadd(reg_b, __float2half(2.0f));
    reg_c = __hadd(reg_c, __float2half(3.0f));
    reg_d = __hadd(reg_d, __float2half(4.0f));

    // å­˜å‚¨é˜¶æ®µ
    output[i] = reg_a;
    output[i + 1] = reg_b;
    output[i + 2] = reg_c;
    output[i + 3] = reg_d;
  }
}
```

#### 3.3.2 æ•°å­¦å‡½æ•°ä¼˜åŒ–
```cpp
// ä½¿ç”¨ç¡¬ä»¶åŠ é€Ÿçš„æ•°å­¦å‡½æ•°
__global__ void math_optimized_kernel(float* input, float* output, int N) {
  int tid = blockIdx.x * blockDim.x + threadIdx.x;

  if (tid < N) {
    float val = input[tid];

    // ä½¿ç”¨ç¡¬ä»¶åŠ é€Ÿå‡½æ•°
    float result = __fadd_rn(val, 1.0f);        // ç¡¬ä»¶åŠ æ³•
    result = __fmul_rn(result, 2.0f);           // ç¡¬ä»¶ä¹˜æ³•
    result = __frcp_rn(result);                 // ç¡¬ä»¶å€’æ•°
    result = __fsqrt_rn(result);                // ç¡¬ä»¶å¹³æ–¹æ ¹
    result = __fexpf(result);                   // ç¡¬ä»¶æŒ‡æ•°å‡½æ•°

    output[tid] = result;
  }
}
```

#### 3.3.3 ç²¾åº¦é€‰æ‹©ä¼˜åŒ–
```cpp
// æ ¹æ®ç²¾åº¦éœ€æ±‚é€‰æ‹©åˆé€‚çš„æ•°æ®ç±»å‹
template <typename T>
__global__ void precision_optimized_kernel(T* input, T* output, int N) {
  int tid = blockIdx.x * blockDim.x + threadIdx.x;

  if (tid < N) {
    T val = input[tid];

    // æ ¹æ®æ•°æ®ç±»å‹é€‰æ‹©åˆé€‚çš„è®¡ç®—
    if constexpr (std::is_same_v<T, half>) {
      // FP16: ä½¿ç”¨ half ç²¾åº¦è®¡ç®—
      output[tid] = __hadd(val, __float2half(1.0f));
    } else if constexpr (std::is_same_v<T, float>) {
      // FP32: ä½¿ç”¨ float ç²¾åº¦è®¡ç®—
      output[tid] = val + 1.0f;
    } else if constexpr (std::is_same_v<T, __nv_bfloat16>) {
      // BF16: ä½¿ç”¨ bfloat16 ç²¾åº¦è®¡ç®—
      output[tid] = __hadd(val, __float2bfloat16(1.0f));
    }
  }
}
```

## 4. å…³é”®å®ç°æ€è·¯æ€»ç»“

### 4.1 FlashAttention 2.0 æ ¸å¿ƒæ€æƒ³

#### 4.1.1 åˆ†å—è®¡ç®—ç­–ç•¥
```cpp
// FlashAttention-2 åˆ†å—è®¡ç®—
template <const int Br = 64, const int Bc = 64>
__global__ void flash_attention_2_kernel(half* Q, half* K, half* V, half* O,
                                        int seqlen, int head_dim) {
  // 1. å°† Q åˆ†å—ä¸º [Br, d] å¤§å°
  // 2. å°† K,V åˆ†å—ä¸º [Bc, d] å¤§å°
  // 3. è®¡ç®— Q@K^T -> P[Br, Bc]
  // 4. è®¡ç®— P@V -> O[Br, d]

  // åˆ†å—å¾ªç¯
  for (int q_tile = 0; q_tile < (seqlen + Br - 1) / Br; ++q_tile) {
    for (int kv_tile = 0; kv_tile < (seqlen + Bc - 1) / Bc; ++kv_tile) {
      // åŠ è½½ Q, K, V åˆ†å—
      load_tile(Q, q_tile, Br, head_dim);
      load_tile(K, kv_tile, Bc, head_dim);
      load_tile(V, kv_tile, Bc, head_dim);

      // è®¡ç®— Q@K^T
      compute_qk();

      // Softmax
      apply_softmax();

      // è®¡ç®— P@V
      compute_pv();

      // ç´¯ç§¯ç»“æœ
      accumulate_result();
    }
  }
}
```

#### 4.1.2 åœ¨çº¿è®¡ç®—ç­–ç•¥
```cpp
// åœ¨çº¿ Softmax è®¡ç®—
template <const int Br = 64, const int Bc = 64>
__global__ void online_softmax_kernel(half* P, half* O, int seqlen, int head_dim) {
  // åœ¨çº¿è®¡ç®— softmaxï¼Œé¿å…å­˜å‚¨å®Œæ•´çš„ P çŸ©é˜µ

  for (int kv_tile = 0; kv_tile < (seqlen + Bc - 1) / Bc; ++kv_tile) {
    // 1. è®¡ç®—å½“å‰ tile çš„æœ€å¤§å€¼
    float max_val = compute_tile_max(P);

    // 2. è®¡ç®—æŒ‡æ•°å’Œ
    float sum_exp = compute_tile_exp_sum(P, max_val);

    // 3. åº”ç”¨ softmax
    apply_tile_softmax(P, max_val, sum_exp);

    // 4. ç´¯ç§¯åˆ°è¾“å‡º
    accumulate_tile_to_output(P, O);
  }
}
```

### 4.2 HGEMM ä¼˜åŒ–æŠ€å·§

#### 4.2.1 Tiling ç­–ç•¥
```cpp
// HGEMM Tiling ç­–ç•¥
template <const int BM = 128, const int BN = 128, const int BK = 32>
__global__ void hgemm_tiling_kernel(half* A, half* B, half* C, int M, int N, int K) {
  // 1. å°†å¤§çŸ©é˜µåˆ†å—ä¸ºå°å—
  // A: [M,K] -> [BM,BK] x (M/BM, K/BK)
  // B: [K,N] -> [BK,BN] x (K/BK, N/BN)
  // C: [M,N] -> [BM,BN] x (M/BM, N/BN)

  // 2. å¾ªç¯åˆ†å—
  for (int bk = 0; bk < (K + BK - 1) / BK; ++bk) {
    for (int bn = 0; bn < (N + BN - 1) / BN; ++bn) {
      for (int bm = 0; bm < (M + BM - 1) / BM; ++bm) {
        // åŠ è½½åˆ†å—åˆ°å…±äº«å†…å­˜
        load_block_to_shared(A, bm, bk, BM, BK);
        load_block_to_shared(B, bk, bn, BK, BN);

        __syncthreads();

        // è®¡ç®—åˆ†å—ä¹˜æ³•
        compute_block_multiply(C, bm, bn);

        __syncthreads();
      }
    }
  }
}
```

#### 4.2.2 å¯„å­˜å™¨é‡ç”¨
```cpp
// å¯„å­˜å™¨æ•°æ®é‡ç”¨ç­–ç•¥
template <const int TM = 8, const int TN = 8, const int TK = 16>
__global__ void register_reuse_kernel(half* A, half* B, half* C, int M, int N, int K) {
  // å¯„å­˜å™¨åˆ†å—
  half reg_a[TM][TK];
  half reg_b[TK][TN];
  float reg_c[TM][TN];

  // ä¸»å¾ªç¯
  for (int k = 0; k < K; k += TK) {
    // åŠ è½½åˆ°å¯„å­˜å™¨ (é‡ç”¨å¯„å­˜å™¨)
    #pragma unroll
    for (int i = 0; i < TM; ++i) {
      #pragma unroll
      for (int j = 0; j < TK; ++j) {
        reg_a[i][j] = A[threadIdx.y * TM + i][k + j];
      }
    }

    #pragma unroll
    for (int i = 0; i < TK; ++i) {
      #pragma unroll
      for (int j = 0; j < TN; ++j) {
        reg_b[i][j] = B[k + i][threadIdx.x * TN + j];
      }
    }

    // è®¡ç®— (æœ€å¤§åŒ–å¯„å­˜å™¨é‡ç”¨)
    #pragma unroll
    for (int i = 0; i < TM; ++i) {
      #pragma unroll
      for (int j = 0; j < TN; ++j) {
        #pragma unroll
        for (int kk = 0; kk < TK; ++kk) {
          reg_c[i][j] += __hmul(reg_a[i][kk], reg_b[kk][j]);
        }
      }
    }
  }

  // å­˜å‚¨ç»“æœ
  // ... (omitted for brevity)
}
```

### 4.3 é€šç”¨ä¼˜åŒ–æ¨¡å¼

#### 4.3.1 Launch Bounds ä½¿ç”¨
```cpp
// ä½¿ç”¨ __launch_bounds__ æ§åˆ¶èµ„æºä½¿ç”¨
template <const int MAX_THREADS = 512, const int MIN_BLOCKS = 4>
__global__ void __launch_bounds__(MAX_THREADS, MIN_BLOCKS)
launch_bounds_optimized_kernel(half* input, half* output, int N) {
  // ç¼–è¯‘å™¨ä¼šæ ¹æ®æŒ‡å®šçš„ bounds ä¼˜åŒ–å¯„å­˜å™¨ä½¿ç”¨
  int tid = blockIdx.x * blockDim.x + threadIdx.x;

  if (tid < N) {
    // å‡å°‘å¯„å­˜å™¨ä½¿ç”¨ä»¥æé«˜ occupancy
    half val = input[tid];
    val = __hadd(val, __float2half(1.0f));
    output[tid] = val;
  }
}
```

#### 4.3.2 å†…å­˜é¢„å–
```cpp
// å†…å­˜é¢„å–ç­–ç•¥
__global__ void prefetch_kernel(half* input, half* output, int N) {
  // é¢„å–ä¸‹ä¸€å—æ•°æ®
  if (threadIdx.x == 0) {
    __builtin_prefetch(&input[blockIdx.x * BLOCK_SIZE + 1024], 0, 1);
  }

  __syncthreads();

  // å¤„ç†å½“å‰å—
  int idx = blockIdx.x * BLOCK_SIZE + threadIdx.x;
  if (idx < N) {
    output[idx] = __hadd(input[idx], __float2half(1.0f));
  }
}
```

#### 4.3.3 åˆ†æ”¯æ¶ˆé™¤
```cpp
// ä½¿ç”¨ predication æ¶ˆé™¤åˆ†æ”¯
__global__ void branch_elimination_kernel(half* input, half* output, int N) {
  int tid = blockIdx.x * blockDim.x + threadIdx.x;

  if (tid < N) {
    half val = input[tid];
    half threshold = __float2half(0.5f);

    // ä½¿ç”¨ predication è€Œéæ¡ä»¶åˆ†æ”¯
    half result = __hgt(val, threshold) ? val : __float2half(0.0f);

    output[tid] = result;
  }
}
```

## 5. å®é™…æ€§èƒ½æ•°æ®

### 5.1 ä¸åŒä¼˜åŒ–çº§åˆ«çš„æ€§èƒ½å¯¹æ¯”

| ä¼˜åŒ–çº§åˆ« | ä»£è¡¨ kernels | æ€§èƒ½æå‡ | å…³é”®æŠ€æœ¯ |
|---------|-------------|---------|---------|
| Basic | åŸºç¡€å®ç° | 1x (åŸºå‡†) | ç›´æ¥å®ç° |
| Easy | elementwise, relu | 2-3x | å‘é‡åŒ–è®¿é—® |
| Medium | layer_norm, softmax | 5-10x | å…±äº«å†…å­˜, åˆ†å—å½’çº¦ |
| Hard | HGEMM, SGEMM | 10-20x | Tensor Core, å¯„å­˜å™¨ä¼˜åŒ– |
| Hard+ | FlashAttention-2 | 20-50x | å¤šé˜¶æ®µæµæ°´çº¿, å¼‚æ­¥æ‹·è´ |
| Hard++ | FFPA Attention | 50-100x | O(1) SRAM, é«˜çº§ swizzle |

### 5.2 å®é™…ç¡¬ä»¶æ€§èƒ½æ•°æ®

#### 5.2.1 HGEMM æ€§èƒ½
- **NVIDIA L20**: è¾¾åˆ° cuBLAS æ€§èƒ½çš„ 98%~100%
- **RTX 4090**: è¾¾åˆ° cuBLAS æ€§èƒ½çš„ 99%~100%
- **RTX 3080 Laptop**: è¾¾åˆ° cuBLAS æ€§èƒ½çš„ 98%~99%

#### 5.2.2 FlashAttention-2 æ€§èƒ½
- **å°è§„æ¨¡ attention** (Bâ‰¤4, Hâ‰¤48, SeqLenâ‰¤8192, Dâ‰¤64):
  - æ¯”å®˜æ–¹ FA2 å¿« **1.5x**
  - RTX 3080 Laptop: 55 TFLOPS (D=64)
- **å¤§è§„æ¨¡ attention**:
  - ä»æœ‰æ€§èƒ½å·®è·ï¼Œæ­£åœ¨ä¼˜åŒ–ä¸­

#### 5.2.3 FFPA Attention æ€§èƒ½
- **æ¯” SDPA å¿« 1.8x~3x**
- **O(1) SRAM å¤æ‚åº¦**: æ”¯æŒå¤§ head_dim (256+)
- **L20**: ~1.9xâ†‘ vs SDPA EA
- **A30**: ~1.8xâ†‘ vs SDPA EA
- **RTX 4090**: ~2.1xâ†‘ vs SDPA EA

### 5.3 ä¼˜åŒ–æ•ˆæœåˆ†æ

#### 5.3.1 å†…å­˜å¸¦å®½åˆ©ç”¨
- **åŸºç¡€ç‰ˆæœ¬**: 30-40% å¸¦å®½åˆ©ç”¨ç‡
- **Easy ä¼˜åŒ–**: 50-60% å¸¦å®½åˆ©ç”¨ç‡
- **Medium ä¼˜åŒ–**: 70-80% å¸¦å®½åˆ©ç”¨ç‡
- **Hard ä¼˜åŒ–**: 85-95% å¸¦å®½åˆ©ç”¨ç‡
- **Hard+ ä¼˜åŒ–**: 95%+ å¸¦å®½åˆ©ç”¨ç‡

#### 5.3.2 è®¡ç®—å•å…ƒåˆ©ç”¨ç‡
- **åŸºç¡€ç‰ˆæœ¬**: 20-30% è®¡ç®—åˆ©ç”¨ç‡
- **Easy ä¼˜åŒ–**: 40-50% è®¡ç®—åˆ©ç”¨ç‡
- **Medium ä¼˜åŒ–**: 60-70% è®¡ç®—åˆ©ç”¨ç‡
- **Hard ä¼˜åŒ–**: 80-90% è®¡ç®—åˆ©ç”¨ç‡ (Tensor Core)
- **Hard+ ä¼˜åŒ–**: 90%+ è®¡ç®—åˆ©ç”¨ç‡

### 5.4 æ€§èƒ½ç“¶é¢ˆåˆ†æ

#### 5.4.1 å†…å­˜ç“¶é¢ˆ
- **å…¨å±€å†…å­˜å»¶è¿Ÿ**: é€šè¿‡æµæ°´çº¿å’Œå¼‚æ­¥æ‹·è´ç¼“è§£
- **å…±äº«å†…å­˜ bank conflicts**: é€šè¿‡ swizzle æ¶ˆé™¤
- **å¯„å­˜å™¨å‹åŠ›**: é€šè¿‡åˆ†å—å’Œå¤ç”¨ç¼“è§£

#### 5.4.2 è®¡ç®—ç“¶é¢ˆ
- **æŒ‡ä»¤ååé‡**: é€šè¿‡ Tensor Core å’Œå‘é‡åŒ–æå‡
- **åˆ†æ”¯é¢„æµ‹**: é€šè¿‡ predication æ¶ˆé™¤åˆ†æ”¯
- **æ•°å€¼ç²¾åº¦**: é€šè¿‡æ··åˆç²¾åº¦å¹³è¡¡ç²¾åº¦å’Œæ€§èƒ½

## 6. æ€»ç»“

LeetCUDA é¡¹ç›®æä¾›äº†ä»åŸºç¡€åˆ°é«˜çº§çš„å®Œæ•´ CUDA kernel ä¼˜åŒ–å­¦ä¹ è·¯å¾„ï¼Œæ¶µç›–äº†ç°ä»£ GPU ç¼–ç¨‹çš„æœ€ä½³å®è·µã€‚é€šè¿‡ç³»ç»Ÿæ€§çš„ä¼˜åŒ–ç­–ç•¥ï¼Œå¯ä»¥å®ç° 2-100x çš„æ€§èƒ½æå‡ã€‚

### 6.1 ä¼˜åŒ–åŸåˆ™
1. **æ¸è¿›å¼ä¼˜åŒ–**: ä»ç®€å•åˆ°å¤æ‚ï¼Œé€æ­¥åº”ç”¨ä¼˜åŒ–æŠ€æœ¯
2. **ç¡¬ä»¶æ„ŸçŸ¥**: æ ¹æ® GPU æ¶æ„ç‰¹æ€§é€‰æ‹©åˆé€‚çš„ä¼˜åŒ–ç­–ç•¥
3. **å¹³è¡¡åŸåˆ™**: åœ¨å†…å­˜ã€è®¡ç®—ã€å¹¶è¡Œåº¦ä¹‹é—´æ‰¾åˆ°æœ€ä½³å¹³è¡¡ç‚¹
4. **å®æµ‹éªŒè¯**: æ‰€æœ‰ä¼˜åŒ–éƒ½éœ€è¦é€šè¿‡å®é™…æ€§èƒ½æµ‹è¯•éªŒè¯

### 6.2 å­¦ä¹ å»ºè®®
1. **ä» Easy å¼€å§‹**: å…ˆæŒæ¡åŸºç¡€çš„å‘é‡åŒ–å’Œå†…å­˜å¯¹é½
2. **ç†è§£åŸç†**: æ·±å…¥ç†è§£æ¯ç§ä¼˜åŒ–æŠ€æœ¯çš„åŸç†å’Œé€‚ç”¨åœºæ™¯
3. **å®è·µéªŒè¯**: é€šè¿‡å®é™…ä»£ç å®ç°å’Œæ€§èƒ½æµ‹è¯•å·©å›ºç†è§£
4. **æŒç»­å­¦ä¹ **: å…³æ³¨æœ€æ–°çš„ GPU æ¶æ„å’Œä¼˜åŒ–æŠ€æœ¯

### 6.3 åº”ç”¨åœºæ™¯
- **æ·±åº¦å­¦ä¹ **: ç¥ç»ç½‘ç»œç®—å­ä¼˜åŒ–
- **ç§‘å­¦è®¡ç®—**: çŸ©é˜µè¿ç®—ã€æ•°å€¼æ¨¡æ‹Ÿ
- **å›¾å½¢å¤„ç†**: å›¾åƒæ»¤æ³¢ã€å‡ ä½•å˜æ¢
- **æ•°æ®åˆ†æ**: å¤§è§„æ¨¡æ•°æ®å¤„ç†ã€ç»Ÿè®¡è®¡ç®—

è¿™ä¸ªä¼˜åŒ–æ–¹æ³•æ€»ç»“ä¸º CUDA é«˜æ€§èƒ½ç¼–ç¨‹æä¾›äº†å…¨é¢çš„å‚è€ƒï¼Œå¸®åŠ©å¼€å‘è€…ç†è§£å’Œå®ç°é«˜æ•ˆçš„ GPU ä»£ç ã€‚